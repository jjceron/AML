%%%%%%%%%% 25/08
% \textcolor{red}{Clase: 25/08/205}

\section{Clustering}

% content
% K-means algorithm
% Expectation-Maximization for Gaussian Mixture

En este módulo, estudiamos modelos de aprendizaje no supervisado. Esto significa que nuestros conjuntos de datos no contienen etiquetas. Por lo tanto, tenemos un conjunto de datos que consiste únicamente en atributos de entrada \([x_1,x_2,...,x_N]\), donde cada \(x_i \in \mathbb{R}^P\). El objetivo es asignar cada muestra \(x\) a uno de los \(K\) clústeres.

\begin{figure}[h]
  \centering
  \newcommand{\figscale}{1.35} 
  \scalebox{\figscale}{\input{tikz/fig_03.tex}}
\end{figure}

\subsection{Fundamentos de \texorpdfstring{\(K\)}{K}-means}
Los grupos se eligen de manera que la distancia entre puntos dentro del mismo grupo sea menor que la distancia entre puntos en diferentes grupos. Para lograr esto, es útil definir el vector \(\boldsymbol{\mu}_{k}\in\mathbb{R}^{P}\), que representa el centro del \(k\)-ésimo grupo. De esta manera, fijamos un conjunto de vectores \({\boldsymbol{\mu}_{k}}_{k=1}^{K}\) tal que la suma de las distancias desde cada punto a su \(\mu_{k}\) más cercano sea lo más pequeña posible.

\begin{figure}[h]
    \centering
    \newcommand{\figscale}{1.35}
    \scalebox{\figscale}{\input{tikz/fig_04.tex}}
\end{figure}

Para cada muestra \(x_n\), definimos un vector binario \(r_n\), compuesto por \(K\) valores \(r_{nk}\), donde \(r_{nk} = 1\) si la instancia \(x_n\) pertenece al grupo \(k\), y \(r_{nk}=0\) en caso contrario. Cabe destacar que cada muestra \(x_n\) solo puede pertenecer a uno de los \(K\) grupos. Por ejemplo, si \(K=4\) grupos y la instancia \(x_n\) pertenece al grupo 3, el vector \(r_n\) será \(r_n = [0,0,1,0]\).

%%%%==== the part above but Spanish, plain text, scientific and following the document context 

El objetivo es encontrar los valores de \(r_{nk}\) y \(\boldsymbol{\mu}_{k}\) que minimicen la distancia entre cada punto \(x_{n}\) y su centro correspondiente \(\mu_{k}\). Matemáticamente, esto se expresa como la minimización de la siguiente función objetivo:

\[
J(\boldsymbol{\mu},\boldsymbol{r}) = \sum_{n=1}^{N} \sum_{k=1}^{K} r_{nk} \| \boldsymbol{x}_n - \boldsymbol{\mu}_k \|^2
\]

Los valores de \(r_{nk}\) y \(\boldsymbol{\mu}_{k}\) se pueden estimar utilizando un enfoque iterativo. El algoritmo \(K\)-means sigue los siguientes pasos:

\begin{algorithm}[ht]
\caption{\(K\)-means}
\SetAlgoLined
Seleccionar $K$ instancias como centroides iniciales $\boldsymbol{\mu}_k$\;
\Repeat{Se cumpla algún criterio de convergencia}{
    \For{$i \leftarrow 1, N$}{
        Para cada $\boldsymbol{x}_n$, definir su vector $\boldsymbol{r}_n$ con los elementos $r_{nk}$:
        $r_{nk} = 
        \begin{cases}
        1, & \text{si la distancia entre $\boldsymbol{x}_n$ y $\boldsymbol{\mu}_k$ es la menor} \\
        0, & \text{en caso contrario.}
        \end{cases}$
    } 
    Actualizar el valor de los centroides: $\displaystyle\boldsymbol{\mu}_k = \frac{\sum_{n=1}^{N} r_{nk}\boldsymbol{x}_n}{\sum_{n=1}^{N} r_{nk}}$\;
}
\end{algorithm}

La función objetivo $J(\boldsymbol{\mu},\boldsymbol{r})$ actúa como una \textbf{función de distorsión} que cuantifica la suma total de distancias cuadradas entre los puntos de datos y sus centroides asignados. Es importante destacar que el algoritmo \(K\)-means puede interpretarse como un método de \textbf{descenso coordinado}, donde se alternan dos pasos: la actualización de las asignaciones $\boldsymbol{r}$ mientras se mantienen fijos los centroides $\boldsymbol{\mu}$, y la actualización de los centroides $\boldsymbol{\mu}$ mientras se mantienen fijas las asignaciones $\boldsymbol{r}$. Durante la ejecución del algoritmo, el valor de $J$ decrece monótonamente hasta alcanzar la convergencia; sin embargo, se debe tener en cuenta que $J$ es una \textbf{función no convexa}, lo que implica que K-means puede converger a un mínimo local en lugar del óptimo global. Para mitigar este problema, una estrategia práctica comúnmente empleada consiste en ejecutar el algoritmo varias veces con diferentes inicializaciones aleatorias de centroides y seleccionar la agrupación que presente el menor valor de distorsión $J(\boldsymbol{\mu},\boldsymbol{r})$.

Una importante limitación del algoritmo \(K\)-means es que solo puede capturar grupos con formas circulares (o hiperesféricas en dimensiones superiores). Esto se debe a que la función objetivo se basa únicamente en la distancia euclidiana entre los puntos y los centroides, lo que favorece agrupaciones con forma circular. Sin embargo, en aplicaciones reales, los datos pueden presentar estructuras más complejas con formas elongadas, no lineales o con densidades variables, donde \(K\)-means no resulta adecuado. Esta limitación es precisamente una de las motivaciones para explorar modelos más flexibles como los GMM. Otro inconveniente es que la estimación que realiza \(K\)-means es \textbf{determinista}, lo que significa que cada punto se asigna de manera definitiva a un solo clúster, constituyendo lo que se conoce como \textbf{clasificación dura}. En situaciones donde los datos presentan solapamientos entre grupos o incertidumbre en las asignaciones, este enfoque puede no ser el más adecuado. En ciertos contextos analíticos es necesaria una \textbf{estimación suave} de las asignaciones, donde cada punto tenga una probabilidad de pertenecer a cada uno de los \(K\) clústeres predefinidos en lugar de una asignación binaria y definitiva. Este enfoque probabilístico permite capturar la incertidumbre inherente en la asignación de puntos que se encuentran en las fronteras entre clústeres o en regiones de solapamiento. Este es otro aspecto que los GMM abordan de manera más efectiva, como veremos a continuación.


\subsection{Modelos de Mezcla Gaussiana (GMM)}

Los Modelos de Mezcla Gaussiana (GMM) son una extensión del algoritmo \(K\)-means. En lugar de asignar cada punto a un solo clúster, GMM permite que cada punto tenga una probabilidad de pertenecer a cada clúster. Esto se logra modelando los datos como una mezcla de varias distribuciones gaussianas.

Cada clúster \(k\) se representa mediante una distribución gaussiana con media \(\boldsymbol{\mu}_k\) y covarianza \(\boldsymbol{\Sigma}_k\). La probabilidad de que un punto \(x_n\) pertenezca al clúster \(k\) se calcula utilizando la función de densidad de la distribución gaussiana:

\[
p(\boldsymbol{x}) = \mathcal{N}(\boldsymbol{x} | \boldsymbol{\mu}, \boldsymbol{\Sigma}
)=\frac{1}{(2\pi)^{P} |\boldsymbol{\Sigma}_k|^{1/2}} \exp\left(-\frac{1}{2} (\boldsymbol{x} - \boldsymbol{\mu})^\mathsf{T} \boldsymbol{\Sigma}^{-1} (\boldsymbol{x} - \boldsymbol{\mu})\right),
\]



